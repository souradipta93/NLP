{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "gridsearch_topic_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souradipta93/NLP/blob/main/gridsearch_topic_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw_gL4E3En6f"
      },
      "source": [
        "# LDA in Python â€“ How to grid search best topic models?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Q7hHDFEn6i"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve0Atx9cEn6j"
      },
      "source": [
        "# Sklearn\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "#remove stopwords\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_co36wa1En6k"
      },
      "source": [
        "df = pd.read_csv('drug.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM3sjTdjEn6k",
        "outputId": "0ad5d2de-a1e4-403c-d018-b380c8b2e340"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>urlDrugName</th>\n",
              "      <th>rating</th>\n",
              "      <th>Review</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>enalapril</td>\n",
              "      <td>4</td>\n",
              "      <td>enalapril management of congestive heart failu...</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ortho-tri-cyclen</td>\n",
              "      <td>1</td>\n",
              "      <td>ortho-tri-cyclen birth prevention - Although t...</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ponstel</td>\n",
              "      <td>10</td>\n",
              "      <td>ponstel menstrual cramps - I was used to havin...</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>prilosec</td>\n",
              "      <td>3</td>\n",
              "      <td>prilosec acid reflux - The acid reflux went aw...</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lyrica</td>\n",
              "      <td>2</td>\n",
              "      <td>lyrica fibromyalgia - I think that the Lyrica ...</td>\n",
              "      <td>Low</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        urlDrugName  rating  \\\n",
              "0         enalapril       4   \n",
              "1  ortho-tri-cyclen       1   \n",
              "2           ponstel      10   \n",
              "3          prilosec       3   \n",
              "4            lyrica       2   \n",
              "\n",
              "                                              Review score  \n",
              "0  enalapril management of congestive heart failu...   Low  \n",
              "1  ortho-tri-cyclen birth prevention - Although t...   Low  \n",
              "2  ponstel menstrual cramps - I was used to havin...  high  \n",
              "3  prilosec acid reflux - The acid reflux went aw...   Low  \n",
              "4  lyrica fibromyalgia - I think that the Lyrica ...   Low  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN2Q1Mk0En6l"
      },
      "source": [
        "#Adding custom stop words\n",
        "new_words = ['http','bit','ly','rt','com','via', 'could', 'would', 'said', 'told', 'yet', 'even', 'shall','let',\n",
        "            'one', 'never', 'might', 'upon', 'first', 'day', 'either', 'rather', 'thing', 'must', 'saw', 'like', 'know',\n",
        "            'time', 'thought', 'made', 'found', 'seemed', 'year', 'mr', 'also', 'last', 'two', 'say', 'make', 'get',\n",
        "            'back', 'take', 'away', 'drug', 'mg', 'side', 'effect', 'medication', 'pill']\n",
        "stop_words = stop_words.union(new_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by9FdxexEn6m"
      },
      "source": [
        "#Text pre-processing\n",
        "corpus = []\n",
        "for i in range(0, df.shape[0]):\n",
        "    #Remove punctuations\n",
        "    text = re.sub('[^a-zA-Z]', ' ', df['Review'][i])\n",
        "    \n",
        "    #Convert to lowercase\n",
        "    text = text.lower()\n",
        "    ##Convert to list from string\n",
        "    text = text.split()\n",
        "    ##Lemmatizing\n",
        "    lm = WordNetLemmatizer() \n",
        "       \n",
        "    \n",
        "    text = [lm.lemmatize(word) for word in text if not word in stop_words] \n",
        "    text = \" \".join(text)\n",
        "    corpus.append(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ci0vAC1En6n"
      },
      "source": [
        "#Most frequently occuring words\n",
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer(stop_words=stop_words, ngram_range=(1,1), max_df=0.7).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UY0vL3qEn6n",
        "outputId": "f931bd96-e944-48a5-ad85-2061f49c3c95"
      },
      "source": [
        "#Convert most freq words to dataframe for plotting bar plot\n",
        "top_words = get_top_n_words(corpus, n=20)\n",
        "top_df = pd.DataFrame(top_words)\n",
        "top_df.columns=[\"Word\", \"Freq\"]\n",
        "top_df.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>taking</td>\n",
              "      <td>2278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pain</td>\n",
              "      <td>2038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>week</td>\n",
              "      <td>1762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>month</td>\n",
              "      <td>1551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>treatment</td>\n",
              "      <td>1402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>skin</td>\n",
              "      <td>1354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>depression</td>\n",
              "      <td>1317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>took</td>\n",
              "      <td>1264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>sleep</td>\n",
              "      <td>1101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>night</td>\n",
              "      <td>1093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>doctor</td>\n",
              "      <td>1074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>feel</td>\n",
              "      <td>1048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>started</td>\n",
              "      <td>1017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>acne</td>\n",
              "      <td>972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>felt</td>\n",
              "      <td>931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>dose</td>\n",
              "      <td>925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>anxiety</td>\n",
              "      <td>924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>hour</td>\n",
              "      <td>910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>symptom</td>\n",
              "      <td>910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>work</td>\n",
              "      <td>908</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Word  Freq\n",
              "0       taking  2278\n",
              "1         pain  2038\n",
              "2         week  1762\n",
              "3        month  1551\n",
              "4    treatment  1402\n",
              "5         skin  1354\n",
              "6   depression  1317\n",
              "7         took  1264\n",
              "8        sleep  1101\n",
              "9        night  1093\n",
              "10      doctor  1074\n",
              "11        feel  1048\n",
              "12     started  1017\n",
              "13        acne   972\n",
              "14        felt   931\n",
              "15        dose   925\n",
              "16     anxiety   924\n",
              "17        hour   910\n",
              "18     symptom   910\n",
              "19        work   908"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "talYf0S1En6n"
      },
      "source": [
        "vectorizer = CountVectorizer(analyzer='word',\n",
        "                             min_df=0.001,\n",
        "                             stop_words=stop_words,\n",
        "                             token_pattern='[a-zA-Z]{3,}',\n",
        "                            ngram_range=(1,1))\n",
        "data_vectorized = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXSuTGDUEn6o",
        "outputId": "84c9741e-84d4-41d9-aca5-7843e9407189"
      },
      "source": [
        "print(data_vectorized.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4143, 3884)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVA32l4BEn6o"
      },
      "source": [
        "# Materialize the sparse data\n",
        "data_dense = data_vectorized.todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ-uUp4vEn6p",
        "outputId": "0d41f4f1-f267-42d7-928f-ed330ac99b80"
      },
      "source": [
        "# Compute Sparsity = Percentage of Non-Zero cells\n",
        "print(\"Sparsity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sparsity:  1.0230239583698435 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGWEc5kBEn6p",
        "outputId": "6e3c4656-1980-483d-d62a-52818f16851e"
      },
      "source": [
        "# Build a Latent Dirichlet Allocation Model\n",
        "lda_model = LatentDirichletAllocation(n_components=5, \n",
        "                                      max_iter=10, \n",
        "                                      learning_method='online',\n",
        "                                     random_state=123,\n",
        "                                     batch_size=128,\n",
        "                                     evaluate_every=-1,\n",
        "                                     n_jobs=-1)\n",
        "\n",
        "lda_output = lda_model.fit_transform(data_vectorized)\n",
        "\n",
        "print(lda_output.shape)  # (NO_DOCUMENTS, NO_TOPICS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4143, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eXJg-I-En6p"
      },
      "source": [
        "### Let us look at the top 10 words of each topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwSfChcxEn6q",
        "outputId": "345998d5-6701-4aa5-f53a-eb1abc18c953"
      },
      "source": [
        "n_top_words = 8\n",
        "\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "  print(\"Topic {}:\".format(topic_idx), end = ' ')\n",
        "  print(\" \".join([vectorizer.get_feature_names()[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 0: infection taking symptom pressure took allergy treatment doctor\n",
            "Topic 1: pain headache migraine hour severe taking nausea relief\n",
            "Topic 2: depression taking sleep feel anxiety week night felt\n",
            "Topic 3: period month blood control level taking week patient\n",
            "Topic 4: skin acne face use hair dry treatment month\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4kPjwlsEn6q"
      },
      "source": [
        "### Add custom stopwords and repeat pre-processing for better topic word mix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWpK3yH2En6q",
        "outputId": "d1ace73b-2440-46bd-8458-99750563fc63"
      },
      "source": [
        "# Log Likelyhood: Higher the better\n",
        "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
        "\n",
        "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
        "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Log Likelihood:  -1476992.192972205\n",
            "Perplexity:  1231.1485239208414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzfeKpDXEn6r"
      },
      "source": [
        "### Setting up the grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OJXBDYnEn6r",
        "outputId": "3714fab3-c152-49a9-dbf7-9a84fceb9a6b"
      },
      "source": [
        "# Define Search Param\n",
        "search_params = {'n_components': [3,4,5], 'learning_method':['online','batch']}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params, verbose=2)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(data_vectorized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "[CV] learning_method=online, n_components=3 ..........................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ........... learning_method=online, n_components=3, total=  22.2s\n",
            "[CV] learning_method=online, n_components=3 ..........................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   22.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] ........... learning_method=online, n_components=3, total=  23.2s\n",
            "[CV] learning_method=online, n_components=3 ..........................\n",
            "[CV] ........... learning_method=online, n_components=3, total=  25.6s\n",
            "[CV] learning_method=online, n_components=3 ..........................\n",
            "[CV] ........... learning_method=online, n_components=3, total=  20.2s\n",
            "[CV] learning_method=online, n_components=3 ..........................\n",
            "[CV] ........... learning_method=online, n_components=3, total=  21.9s\n",
            "[CV] learning_method=online, n_components=4 ..........................\n",
            "[CV] ........... learning_method=online, n_components=4, total=  23.5s\n",
            "[CV] learning_method=online, n_components=4 ..........................\n",
            "[CV] ........... learning_method=online, n_components=4, total=  21.3s\n",
            "[CV] learning_method=online, n_components=4 ..........................\n",
            "[CV] ........... learning_method=online, n_components=4, total=  21.0s\n",
            "[CV] learning_method=online, n_components=4 ..........................\n",
            "[CV] ........... learning_method=online, n_components=4, total=  23.3s\n",
            "[CV] learning_method=online, n_components=4 ..........................\n",
            "[CV] ........... learning_method=online, n_components=4, total=  23.6s\n",
            "[CV] learning_method=online, n_components=5 ..........................\n",
            "[CV] ........... learning_method=online, n_components=5, total=  20.8s\n",
            "[CV] learning_method=online, n_components=5 ..........................\n",
            "[CV] ........... learning_method=online, n_components=5, total=  21.1s\n",
            "[CV] learning_method=online, n_components=5 ..........................\n",
            "[CV] ........... learning_method=online, n_components=5, total=  20.9s\n",
            "[CV] learning_method=online, n_components=5 ..........................\n",
            "[CV] ........... learning_method=online, n_components=5, total=  22.3s\n",
            "[CV] learning_method=online, n_components=5 ..........................\n",
            "[CV] ........... learning_method=online, n_components=5, total=  20.2s\n",
            "[CV] learning_method=batch, n_components=3 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=3, total=  35.3s\n",
            "[CV] learning_method=batch, n_components=3 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=3, total=  31.1s\n",
            "[CV] learning_method=batch, n_components=3 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=3, total=  33.4s\n",
            "[CV] learning_method=batch, n_components=3 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=3, total=  29.0s\n",
            "[CV] learning_method=batch, n_components=3 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=3, total=  30.7s\n",
            "[CV] learning_method=batch, n_components=4 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=4, total=  35.9s\n",
            "[CV] learning_method=batch, n_components=4 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=4, total=  29.8s\n",
            "[CV] learning_method=batch, n_components=4 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=4, total=  33.1s\n",
            "[CV] learning_method=batch, n_components=4 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=4, total=  30.4s\n",
            "[CV] learning_method=batch, n_components=4 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=4, total=  31.0s\n",
            "[CV] learning_method=batch, n_components=5 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=5, total=  32.6s\n",
            "[CV] learning_method=batch, n_components=5 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=5, total=  32.2s\n",
            "[CV] learning_method=batch, n_components=5 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=5, total=  28.4s\n",
            "[CV] learning_method=batch, n_components=5 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=5, total=  30.2s\n",
            "[CV] learning_method=batch, n_components=5 ...........................\n",
            "[CV] ............ learning_method=batch, n_components=5, total=  31.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 13.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=LatentDirichletAllocation(),\n",
              "             param_grid={'learning_method': ['online', 'batch'],\n",
              "                         'n_components': [3, 4, 5]},\n",
              "             verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwGc2rFCEn6r"
      },
      "source": [
        "### Best model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLKdmcWEEn6s",
        "outputId": "d868c6b3-25ac-433b-8c24-896b2ec53218"
      },
      "source": [
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Model's Params:  {'learning_method': 'online', 'n_components': 3}\n",
            "Best Log Likelihood Score:  -313149.01337389974\n",
            "Model Perplexity:  1258.1070683100563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RdKK7crEn6s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}